{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6133, 0.0249, 0.4206],\n",
      "        [0.0442, 0.7712, 0.8495],\n",
      "        [0.7180, 0.8101, 0.2474],\n",
      "        [0.6603, 0.4419, 0.9931],\n",
      "        [0.9735, 0.2315, 0.2177]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irmas Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "irmas_path = '../IRMAS/'\n",
    "irmas_train = f'{irmas_path}/IRMAS-TrainingData'\n",
    "train_df = pd.DataFrame(columns=['filename', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Irmas datset, I wont be using process_tracks script nor silence removal and sampling operations, since every audio file has only 2 seconds\n",
    "> Annotations: The annotation of the predominant instrument of each excerpt is both in the name of the containing folder, and in the file name: cello (cel), clarinet (cla), flute (flu), acoustic guitar (gac), electric guitar (gel), organ (org), piano (pia), saxophone (sax), trumpet (tru), violin (vio), and human singing voice (voi).\n",
    "<br><br>\n",
    "> The number of files per instrument are: cel(388), cla(505), flu(451), gac(637), gel(760), org(682), pia(721), sax(626), tru(577), vio(580), voi(778). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original dataset, we include only the annotations of the considered pitched instruments:  cello (cel), clarinet (cla), flute (flu), acoustic guitar (gac), electric guitar (gel), organ (org), piano (pia), saxophone (sax), trumpet (tru), violin (vio), and human singing voice (voi). Other instruments such as bass, percussion or sections such as brass or strings are thus not included in the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_filename(filename):\n",
    "    return label_from_acronym(filename.strip('[]')[:2])\n",
    "\n",
    "def label_from_acronym(acronym):\n",
    "    label_mapping = {\n",
    "        'cel': 'cello',\n",
    "        'cla': 'clarinet',\n",
    "        'flu': 'flute',\n",
    "        'gac': 'acoustic guitar',\n",
    "        'gel': 'electric guitar',\n",
    "        'org': 'organ',\n",
    "        'pia': 'piano',\n",
    "        'sax': 'saxophone',\n",
    "        'tru': 'trumpet',\n",
    "        'vio': 'violin',\n",
    "        'voi': 'human singing voice'\n",
    "    }\n",
    "    return label_mapping[acronym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(irmas_train):\n",
    "    \n",
    "    if filename.endswith('.wav'):\n",
    "        file_path = os.path.join(irmas_train, filename)\n",
    "        train_df = train_df.append({'filename': filename, 'label': label_from_filename(filename)}, ignore_index=True)\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
